{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/local/miniconda3/envs/dl/lib/python3.6/site-packages/ipykernel_launcher.py']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import data_util\n",
    "import similarity\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# data loading params\n",
    "tf.compat.v1.flags.DEFINE_string(\n",
    "    \"knowledge_file\", \"data/knowledge.txt\", \"Knowledge data.\")\n",
    "tf.compat.v1.flags.DEFINE_string(\n",
    "    \"train_file\", \"data/train.txt\", \"Training data.\")\n",
    "tf.compat.v1.flags.DEFINE_string(\n",
    "    \"test_file\", \"data/test.txt\", \"Test data.\")\n",
    "tf.compat.v1.flags.DEFINE_string(\n",
    "    \"stop_words_file\", \"data/stop_words.txt\", \"Stop words.\")\n",
    "\n",
    "# result & model save params\n",
    "tf.compat.v1.flags.DEFINE_string(\n",
    "    \"result_file\", \"res/predictRst.score\", \"Predict result.\")\n",
    "tf.compat.v1.flags.DEFINE_string(\n",
    "    \"save_file\", \"res/savedModel\", \"Save model.\")\n",
    "\n",
    "# pre-trained word embedding vectors\n",
    "# Path to embedding file!\n",
    "tf.compat.v1.flags.DEFINE_string(\"embedding_file\", \n",
    "  \"zhwiki_2017_03.sg_50d_1.word2vec\", \n",
    "  \"Embedding vectors.\")\n",
    "\n",
    "# hyperparameters\n",
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "    \"k\", 5, \"K most similarity knowledge (default: 5).\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "    \"rnn_size\", 100, \n",
    "    \"Neurons number of hidden layer in LSTM cell (default: 100).\")\n",
    "tf.compat.v1.flags.DEFINE_float(\n",
    "    \"margin\", 0.1, \"Constant of max-margin loss (default: 0.1).\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "    \"max_grad_norm\", 5, \"Control gradient expansion (default: 5).\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "    \"embedding_dim\", 50, \"Dimensionality of character embedding (default: 50).\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "    \"max_sentence_len\", 100, \"Maximum number of words in a sentence (default: 100).\")\n",
    "tf.compat.v1.flags.DEFINE_float(\n",
    "    \"dropout_keep_prob\", 0.45, \"Dropout keep probability (default: 0.5).\")\n",
    "tf.compat.v1.flags.DEFINE_float(\n",
    "    \"learning_rate\", 0.001, \"Learning rate (default: 0.4).\")\n",
    "tf.compat.v1.flags.DEFINE_float(\n",
    "    \"lr_down_rate\", 0.5, \"Learning rate down rate(default: 0.5).\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "    \"lr_down_times\", 4, \"Learning rate down times (default: 4)\")\n",
    "tf.compat.v1.flags.DEFINE_float(\n",
    "    \"l2_reg_lambda\", 0.1, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# training parameters\n",
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "    \"batch_size\", 512, \"Batch Size (default: 64)\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "    \"num_epochs\", 20, \"Number of training epochs (default: 20)\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "    \"evaluate_every\", 20, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "    \"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.compat.v1.flags.DEFINE_integer(\n",
    "    \"num_checkpoints\", 20, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# gpu parameters\n",
    "tf.compat.v1.flags.DEFINE_float(\n",
    "    \"gpu_mem_usage\", 0.75, \"GPU memory max usage rate (default: 0.75).\")\n",
    "tf.compat.v1.flags.DEFINE_string(\n",
    "    \"gpu_device\", \"/gpu:0\", \"GPU device name.\")\n",
    "\n",
    "# misc parameters\n",
    "tf.compat.v1.flags.DEFINE_boolean(\n",
    "    \"allow_soft_placement\", True, \"Allow device soft device placement.\")\n",
    "\n",
    "#加了一行代码,以适应jupyter\n",
    "tf.compat.v1.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "FLAGS = tf.compat.v1.flags.FLAGS\n",
    "FLAGS(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, word2idx = data_util.load_embedding(FLAGS.embedding_file)\n",
    "stop_words = codecs.open(FLAGS.stop_words_file, 'r', encoding='utf8').readlines()\n",
    "stop_words = [w.strip() for w in stop_words]\n",
    "#similarity.generate_dic_and_corpus(FLAGS.knowledge_file, FLAGS.train_file, \n",
    "#                                    stop_words)\n",
    "train_sim_ixs = similarity.topk_sim_ix(FLAGS.train_file, stop_words, FLAGS.k)\n",
    "test_sim_ixs = similarity.topk_sim_ix(FLAGS.test_file, stop_words, FLAGS.k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.987 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_questions, train_answers, train_labels, train_question_num =        \\\n",
    "    data_util.load_data(FLAGS.knowledge_file, FLAGS.train_file, word2idx, \\\n",
    "    stop_words, train_sim_ixs, FLAGS.max_sentence_len)\n",
    "\n",
    "test_questions, test_answers, test_labels, test_question_num =            \\\n",
    "    data_util.load_data(FLAGS.knowledge_file, FLAGS.test_file, word2idx,  \\\n",
    "    stop_words, test_sim_ixs, FLAGS.max_sentence_len)\n",
    "\n",
    "questions, true_answers, false_answers = [], [], []\n",
    "\n",
    "for q, ta, fa in data_util.training_batch_iter(\n",
    "    train_questions, train_answers, train_labels, \n",
    "    train_question_num, FLAGS.batch_size\n",
    "):\n",
    "    #questions.append(q), true_answers.append(ta), false_answers.append(fa)\n",
    "    #用于分布式训练\n",
    "    questions.extend(q), true_answers.extend(ta), false_answers.extend(fa)\n",
    "\n",
    "t_questions, t_answers = [], []  \n",
    "for q, a in data_util.testing_batch_iter(\n",
    "    test_questions, test_answers, test_question_num, FLAGS.batch_size\n",
    "):\n",
    "    #questions.append(q), true_answers.append(ta), false_answers.append(fa)\n",
    "    #用于分布式训练\n",
    "    t_questions.extend(q), t_answers.extend(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GLOBAL_BATCH_SIZE = FLAGS.batch_size * strategy.num_replicas_in_sync\n",
    "GLOBAL_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((questions, true_answers, false_answers)).batch(GLOBAL_BATCH_SIZE) \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((t_questions, t_answers)).batch(GLOBAL_BATCH_SIZE) \n",
    "\n",
    "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simModel(embedding, embedding_dim, vocab_size, max_sentence_len, \n",
    "    rnn_size, dropout_keep_prob):\n",
    "    \n",
    "    class SimLayer(layers.Layer):\n",
    "        def __init__(self):\n",
    "            super(SimLayer, self).__init__()\n",
    "\n",
    "        def call(self, q, a):\n",
    "            q1 = tf.sqrt(tf.reduce_sum(tf.multiply(q, q), 1))\n",
    "            a1 = tf.sqrt(tf.reduce_sum(tf.multiply(a, a), 1))\n",
    "            mul = tf.reduce_sum(tf.multiply(q, a), 1)\n",
    "            cosSim = tf.divide(mul, tf.multiply(q1, a1))\n",
    "            return cosSim\n",
    "    \n",
    "    bilstm_inputs = layers.Input(shape=(max_sentence_len,))\n",
    "\n",
    "    emb = layers.Embedding(vocab_size, embedding_dim, \n",
    "       weights=[np.asarray(embedding)], trainable=False)  ##初始化词向量\n",
    "    emb_inputs = emb(bilstm_inputs)\n",
    "    \n",
    "    bilstm = layers.Bidirectional(\n",
    "      layers.LSTM(\n",
    "          rnn_size, activation='relu', return_sequences=True,\n",
    "          dropout=dropout_keep_prob, recurrent_dropout=dropout_keep_prob\n",
    "      ))(emb_inputs)\n",
    "\n",
    "    bilstm_max = tf.keras.backend.max(bilstm, axis=1)\n",
    "    bilstm_out = tf.keras.backend.tanh(bilstm_max)\n",
    "    bilstm_model = keras.Model(inputs=bilstm_inputs, outputs=bilstm_out)\n",
    "    bilstm_model.summary()\n",
    "    \n",
    "    q_inputs = layers.Input(shape=(max_sentence_len,))\n",
    "    a_inputs = layers.Input(shape=(max_sentence_len,))\n",
    "    q_bilstm = bilstm_model(q_inputs)\n",
    "    a_bilstm = bilstm_model(a_inputs)\n",
    "    \n",
    "    similarity = SimLayer()(q_bilstm, a_bilstm)\n",
    "    sim_model = keras.Model(inputs=[q_inputs, a_inputs], outputs=similarity)\n",
    "    \n",
    "    sim_model.summary()\n",
    "    return sim_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建检查点目录以存储检查点。\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "  def compute_loss(trueCosSim, falseCosSim, margin): #在一个GLobal_batch下计算损失\n",
    "    zero = tf.fill(tf.shape(trueCosSim), 0.0)\n",
    "    tfMargin = tf.fill(tf.shape(trueCosSim), margin)\n",
    "   \n",
    "    # max-margin losses = max(0, margin - true + false)\n",
    "    per_example_loss = tf.maximum(zero, tf.subtract(tfMargin, \n",
    "      tf.subtract(trueCosSim, falseCosSim))) #[global_batch]\n",
    "    #loss = tf.reduce_sum(losses)\n",
    "    return tf.nn.compute_average_loss(per_example_loss, \n",
    "                                      global_batch_size=GLOBAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必须在'strategy.scope'下创建模型和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 100, 50)           29922700  \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 100, 200)          120800    \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Max_6 (TensorFlo [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Tanh_6 (TensorFl [(None, 200)]             0         \n",
      "=================================================================\n",
      "Total params: 30,043,500\n",
      "Trainable params: 120,800\n",
      "Non-trainable params: 29,922,700\n",
      "_________________________________________________________________\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_12 (Model)                (None, 200)          30043500    input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sim_layer_6 (SimLayer)          (None,)              0           model_12[1][0]                   \n",
      "==================================================================================================\n",
      "Total params: 30,043,500\n",
      "Trainable params: 120,800\n",
      "Non-trainable params: 29,922,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#采用sim_model作为最终模型\n",
    "with strategy.scope():\n",
    "    vocab_size = len(embedding)\n",
    "    sim_model = simModel(embedding, FLAGS.embedding_dim, vocab_size, \n",
    "        FLAGS.max_sentence_len, FLAGS.rnn_size, FLAGS.dropout_keep_prob)\n",
    "    margin = FLAGS.margin\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(lr = FLAGS.learning_rate)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=sim_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    #一个global_batch数据分发到各个机器\n",
    "    def train_step(inputs):\n",
    "        question, trueAnswer, falseAnswer = inputs\n",
    "        #print('train_step',question.shape, trueAnswer.shape, falseAnswer.shape)\n",
    "        question = tf.cast(question, dtype='float32')\n",
    "        trueAnswer = tf.cast(trueAnswer, dtype='float32')\n",
    "        falseAnswer = tf.cast(falseAnswer, dtype='float32')\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            trueCosSim = sim_model((question, trueAnswer))\n",
    "            falseCosSim = sim_model((question, falseAnswer))\n",
    "        \n",
    "            loss = compute_loss(trueCosSim, falseCosSim, margin)\n",
    "        grads = tape.gradient(loss, sim_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, sim_model.trainable_variables))\n",
    "        #loss_metric(loss)\n",
    "        #tf.print('mean loss = %s' % (loss))\n",
    "        return loss\n",
    "    \n",
    "    def test_step(inputs):\n",
    "        test_q, test_a = inputs\n",
    "        test_q = tf.cast(test_q, dtype='float32')\n",
    "        test_a = tf.cast(test_a, dtype='float32')\n",
    "        cosSim = sim_model((test_q, test_a))\n",
    "        return cosSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.09830742329359055\n",
      "evaluation acc:  0.2859244727807749\n",
      "Epoch 2, Loss: 0.09824561327695847\n",
      "evaluation acc:  0.28347229033840116\n",
      "Epoch 3, Loss: 0.09816975146532059\n",
      "evaluation acc:  0.28249141736145167\n",
      "Epoch 4, Loss: 0.09812230616807938\n",
      "evaluation acc:  0.28347229033840116\n",
      "Epoch 5, Loss: 0.09811707586050034\n",
      "evaluation acc:  0.2805296714075527\n",
      "Epoch 6, Loss: 0.09795045852661133\n",
      "evaluation acc:  0.2790583619421285\n",
      "Epoch 7, Loss: 0.0978194922208786\n",
      "evaluation acc:  0.2829818538499264\n",
      "Epoch 8, Loss: 0.09785612672567368\n",
      "evaluation acc:  0.2829818538499264\n",
      "Epoch 9, Loss: 0.09773333370685577\n",
      "evaluation acc:  0.2864149092692496\n",
      "Epoch 10, Loss: 0.09837506711483002\n",
      "evaluation acc:  0.28249141736145167\n",
      "Epoch 11, Loss: 0.09931309521198273\n",
      "evaluation acc:  0.2653261402648357\n",
      "Epoch 12, Loss: 0.09923665970563889\n",
      "evaluation acc:  0.2741539970573811\n",
      "Epoch 13, Loss: 0.09909264743328094\n",
      "evaluation acc:  0.28249141736145167\n",
      "Epoch 14, Loss: 0.09893905371427536\n",
      "evaluation acc:  0.2820009808729769\n",
      "Epoch 15, Loss: 0.09884976595640182\n",
      "evaluation acc:  0.2805296714075527\n",
      "Epoch 16, Loss: 0.0987553596496582\n",
      "evaluation acc:  0.27709661598822954\n",
      "Epoch 17, Loss: 0.09869644790887833\n",
      "evaluation acc:  0.2790583619421285\n",
      "Epoch 18, Loss: 0.09861908107995987\n",
      "evaluation acc:  0.2790583619421285\n",
      "Epoch 19, Loss: 0.09848018735647202\n",
      "evaluation acc:  0.27954879843060326\n",
      "Epoch 20, Loss: 0.09829176962375641\n",
      "evaluation acc:  0.2815105443845022\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    # `experimental_run_v2`将复制提供的计算并使用分布式输入运行它。\n",
    "    @tf.function\n",
    "    #一个global_batch数据分发到各个机器\n",
    "    def distributed_train_step(dataset_inputs):\n",
    "        #分布式训练，返回聚合损失\n",
    "        per_replica_losses = strategy.experimental_run_v2(train_step,\n",
    "          args=(dataset_inputs,))\n",
    "        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                   axis=None)\n",
    "    \n",
    "    #@tf.function\n",
    "    def distributed_test_step(test_dataset):\n",
    "        t_labels = tf.constant(test_labels)\n",
    "        first_time = True\n",
    "        scores = tf.constant(np.array([], dtype='float32'))\n",
    "        for x in test_dataset:\n",
    "            #print(x)\n",
    "            score = strategy.experimental_run_v2(test_step, args=(x,))\n",
    "            scores = tf.concat((scores, score), axis=0)      \n",
    "        cnt = 0\n",
    "        scores = tf.abs(scores)\n",
    "        for test_id in tf.range(test_question_num):    \n",
    "            offset = tf.multiply(test_id, 4)          \n",
    "            predict_true_ix = tf.argmax(scores[offset:tf.add(offset,4)], output_type=offset.dtype)\n",
    "            if t_labels[tf.add(offset,predict_true_ix)] == 1:\n",
    "                cnt += 1\n",
    "        tf.print(\"evaluation acc: \", tf.divide(cnt, test_question_num))\n",
    "\n",
    "    for epoch in range(FLAGS.num_epochs):\n",
    "        # 训练循环\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for x in train_dist_dataset:\n",
    "            batch_loss = distributed_train_step(x)\n",
    "            total_loss += batch_loss\n",
    "            #tf.print('global_batch_loss: ', batch_loss * 1000)\n",
    "            num_batches += 1\n",
    "            train_loss = total_loss / num_batches              \n",
    "        template = (\"Epoch {}, Loss: {}\")\n",
    "        tf.print (template.format(epoch+1, train_loss))\n",
    "        # 测试循环\n",
    "        distributed_test_step(test_dist_dataset)\n",
    "        if epoch % 2 == 0:\n",
    "            checkpoint.save(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
